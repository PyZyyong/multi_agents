import os
import asyncio
import operator
from typing import TypedDict, Annotated, Sequence
from langchain_openai import ChatOpenAI
from langchain_deepseek import ChatDeepSeek
from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph_supervisor import create_supervisor
from langgraph.graph import END, StateGraph, START
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.prebuilt import create_react_agent
from mcp_adapters import tavily_tool, python_repl, get_weather_warning, get_daily_forecast

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "default"

chat_4o_model = ChatOpenAI(
    model="gpt-4o",
    temperature=0.2,
    streaming=True,
    base_url="https://opnai-api.top/v1",
    api_key=os.getenv("OPENAI_API_KEY"),
)

deepseek_model = ChatDeepSeek(
    # deepseek-chat å¯¹åº” DeepSeek-V3ï¼›deepseek-reasoner å¯¹åº” DeepSeek-R1ã€‚
    model="deepseek-chat",
    temperature=0.2,
    streaming=True,
    base_url="https://api.deepseek.com",
    api_key=os.getenv("DEEPSEEK_API_KEY"),
)


def create_agent(llm, tools, tool_message: str, custom_notice: str = ""):
    """åˆ›å»ºä¸€ä¸ªæ™ºèƒ½ä½“ã€‚"""
    # å®šä¹‰æ™ºèƒ½ä½“çš„æç¤ºæ¨¡æ¿ï¼ŒåŒ…å«ç³»ç»Ÿæ¶ˆæ¯å’Œå·¥å…·ä¿¡æ¯
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œå¯ä»¥ä¸å…¶ä»–åŠ©æ‰‹åˆä½œï¼Œä¸€èµ·å¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ã€‚"
                "å¦‚æœä½ æ— æ³•ç‹¬ç«‹å®Œæˆå›ç­”ï¼Œå¯ä»¥å°†ä»»åŠ¡äº¤ç»™å…¶ä»–åŠ©æ‰‹ç»§ç»­å¤„ç†ã€‚"
                "å¦‚æœä½ æœ‰æ»¡è¶³ç”¨æˆ·éœ€æ±‚çš„æœ€ç»ˆç»“æœï¼Œè¯·åœ¨å“åº”å‰åŠ ä¸Š FINAL ANSWER ä»¥ä¾¿ç¨‹åºåœæ­¢ã€‚"
                "\n{custom_notice}\n"
                "ä½ æœ‰ä»¥ä¸‹å·¥å…·å¯ä»¥ä½¿ç”¨: {tool_names}.\n{tool_message}\n\n",
            ),
            MessagesPlaceholder(variable_name="messages"),
        ]
    )

    # å°†ç³»ç»Ÿæ¶ˆæ¯éƒ¨åˆ†å’Œå·¥å…·åç§°æ’å…¥åˆ°æç¤ºæ¨¡æ¿ä¸­
    prompt = prompt.partial(tool_message=tool_message, custom_notice=custom_notice)
    prompt = prompt.partial(tool_names=", ".join([tool.name for tool in tools]))

    return prompt | llm.bind_tools(tools)

checkopint = InMemorySaver()

research_agent = create_react_agent(
    model=deepseek_model,
    tools=[tavily_tool],
    prompt=f"""ä½ æ˜¯ä¸€ä¸ªç ”ç©¶å‘˜åŠ©æ‰‹ï¼Œå¯¹äºä¸äº†è§£çš„é—®é¢˜å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·è¿›è¡Œæœç´¢ï¼š{tavily_tool.name}ï¼Œ\n
    åœ¨ä½¿ç”¨æœç´¢å·¥å…·ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒå¹¶æ˜ç¡®æŸ¥è¯¢å†…å®¹ã€‚ç„¶åï¼Œè¿›è¡Œä¸€æ¬¡æœç´¢ï¼Œä¸€æ¬¡æ€§è§£å†³æŸ¥è¯¢çš„æ‰€æœ‰éœ€æ±‚ã€‚""",
    name="research_assistant"
)

chart_agent = create_react_agent(
    model=deepseek_model,
    tools=[python_repl],
    prompt=f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šå›¾è¡¨ç”Ÿæˆä¸“å®¶ï¼ŒåŸºäºå…¶ä»–æ™ºèƒ½åŠ©æ‰‹æä¾›çš„æ•°æ®ï¼Œç”Ÿæˆå›¾è¡¨ã€‚è¦æ±‚å›¾è¡¨æ¸…æ™°ï¼Œæ˜“äºç†è§£ã€‚\n
    ä½ æœ‰ä»¥ä¸‹å·¥å…·ä½¿ç”¨:{python_repl.name}""",
    name="chart_assistant"
)

weather_agent = create_react_agent(
    model=deepseek_model,
    tools=[get_weather_warning, get_daily_forecast],
    prompt="""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å¤©æ°”æŸ¥è¯¢åŠ©æ‰‹ï¼Œæ ¸å¿ƒä»»åŠ¡æ˜¯é€šè¿‡è°ƒç”¨å†…ç½®å·¥å…·è·å–å®æ—¶æ•°æ®ï¼Œå¹¶è½¬åŒ–ä¸ºç»“æ„æ¸…æ™°çš„å›¾è¡¨ç»“æ„ï¼Œ\n
    å›¾è¡¨ç»“æ„å†…åº”è¯¥è¯¦ç»†çš„è®°å½•å¤©æ°”ä¿¡æ¯çš„å„ä¸ªæŒ‡æ ‡ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£ã€‚éœ€ä¿æŒä¸“ä¸šä¸”å£è¯­åŒ–çš„è¡¨è¾¾ï¼Œå¿…è¦æ—¶ç”¨ç¬¦å·/è¡¨æƒ…è¾…åŠ©ç†è§£ï¼ˆå¦‚ğŸŒ¤ï¸â›ˆï¸ï¼‰ã€‚
    **ä½ å¯ä»¥ä½¿ç”¨çš„å·¥å…·**
    get_weather_warning: æ ¹æ®æä¾›çš„åŸå¸‚åæŸ¥è¯¢å¤©æ°”é¢„è­¦ä¿¡æ¯ã€‚
    get_daily_forecast: æ ¹æ®æä¾›çš„åŸå¸‚åï¼ŒæŸ¥è¯¢æœ€è¿‘æ—¥æœŸçš„å¤©æ°”ä¿¡æ¯å¦‚ä¸€å‘¨ã€ä¸‰å¤©å†…ã€äº”å¤©å†…ã€ä¸€ä¸ªæœˆç­‰ã€‚
    """,
    name="weather_assistant"
)

supervisor_agent = create_supervisor(
    agents=[chart_agent, weather_agent, research_agent],
    model=deepseek_model,
    prompt=(
        """ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ç®¡ç†å‘˜ï¼Œç®¡ç†ä»¥ä¸‹åŠ©æ‰‹ï¼šweather_assistantï¼Œchart_assistantï¼Œ research_assistantã€‚ä»”ç»†åˆ†æç”¨æˆ·çš„éœ€æ±‚ï¼Œå¦‚æœä½ æ— æ³•ç‹¬ç«‹å®Œæˆå›ç­”ï¼Œå¯ä»¥å°†ä»»åŠ¡äº¤ç»™å…¶ä»–åŠ©æ‰‹ç»§ç»­å¤„ç†ã€‚\n
        æ¯”å¦‚ç”¨æˆ·è¦æŸ¥è¯¢å¤©æ°”ä¿¡æ¯ï¼Œä½ åº”è¯¥ä½¿ç”¨weather_assistantï¼Œç”¨æˆ·è¦ç”Ÿæˆå›¾è¡¨ä½ åº”è¯¥ä½¿ç”¨chart_assistantï¼Œå¯¹äºä½ ä¸ç¡®å®šçš„é—®é¢˜ï¼Œä½ åº”è¯¥ä½¿ç”¨research_assistantï¼Œ\n
        ä½ å¯ä»¥åŒæ—¶ä½¿ç”¨ä¸€ä¸ªæˆ–è€…å¤šä¸ªåŠ©æ‰‹åä½œå®Œæˆä»»åŠ¡ã€‚"""
    ),
    supervisor_name='manager',
    output_mode="full_history"
)

supervisor = supervisor_agent.compile(checkpointer=checkopint)

from IPython.display import display, Image
display(
    Image(
        supervisor.get_graph(xray=True).draw_mermaid_png(max_retries=3)
    )
)
if __name__ == "__main__":
    # print(chat_4o_model.invoke("ä½ å¥½"))
    async def main():
        while True:
            user_input = input("User: ")
            if user_input.lower() in ["quit", "exit", "q"]:
                print("Goodbye!")
                break

            async for chunk in supervisor.astream(
                {"messages": ("user", user_input)},
                config={"configurable": {"thread_id": "current_user_id"}}
            ):
                sender = {'manager', 'chart_assistant', 'weather_assistant', 'research_assistant'}
                for s in sender:
                    if messages := chunk.get(s):
                        break
                if messages:
                    last_message = messages['messages'][-1]
                    if not isinstance(last_message, ToolMessage):
                        # print(last_message.pretty_print())
                        print(last_message)

    asyncio.run(main())
